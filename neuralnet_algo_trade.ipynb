{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b26039-68c3-4432-b1e9-dedefbf57f5c",
   "metadata": {},
   "source": [
    "# Why Neural Network\n",
    "\n",
    "Neural Network was leveraged as one of the predictive model options for this project, given that:\n",
    ">\n",
    "> * The algorithm can recognize hidden patterns and correlations in raw data and cluster, and over time continuously learn and improve\n",
    ">\n",
    "> * it provides options and parameters that are adjustable, these parameters can be changed continuously throughout the learning process to improve the overall output result of the model\n",
    "\n",
    "\n",
    "\n",
    "## Data Preparation and Model Training Process\n",
    "\n",
    "The following steps were applied to prepare the data and train the model:\n",
    "\n",
    "1. By Leveraging Alpaca, we extracted trading data for 6 months period, steps taken are as follows:\n",
    ">\n",
    "> * Set Alpaca API key and secret key.\n",
    ">\n",
    "> * Verify that Alpaca key and secret key were correctly loaded.\n",
    ">\n",
    "> * Create the Alpaca API object.\n",
    ">\n",
    "> * Reterived the tarding data for the specific ticker for the period required\n",
    "\n",
    "2. Compile and evaluate a binary classification model using a neural network.\n",
    "\n",
    "3. Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “signal”. The remaining columns should define the features dataset.\n",
    "\n",
    "4. Split the features and target sets into training and testing datasets.\n",
    "\n",
    "5. Use scikit-learn's `StandardScaler` to scale the features data.\n",
    "\n",
    "### Model Evaluation and Training Process Using a Neural Network\n",
    "\n",
    "The following steps were completed as part of the evaluation and Training process:\n",
    "\n",
    "1. Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "2. Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n",
    "\n",
    "3. Utilized small number of epochs when fitting the model\n",
    "\n",
    "3. Evaluate the model using the test data to determine the model’s loss and accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b8f39-a7b3-4ad9-a7e9-edb9cdb2d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "from pathlib import Path\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98a501-4a7b-45c9-b5e0-e466072688cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2cf17-fdba-4adb-9975-786630a64753",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Retrieval from Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bba6e3-e5f8-4a6e-ba3f-6707c07b6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret key\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7f80b-d315-4535-a4cc-6f34eac16fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Alpaca key and secret were correctly loaded\n",
    "print(f\"Alpaca Key type: {type(alpaca_api_key)}\")\n",
    "print(f\"Alpaca Secret Key type: {type(alpaca_secret_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cc371-e4f6-422e-9e78-03fd9f8aadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Alpaca API object\n",
    "project2_alpaca = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fa875-0be7-4eeb-b65f-867afa0f942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"AAPL\"]\n",
    "timeframe = \"1Day\"\n",
    "start = pd.Timestamp(\"2023-01-05\", tz=\"America/New_York\").isoformat()\n",
    "end = pd.Timestamp(\"2023-08-24\", tz=\"America/New_York\").isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5940a79-e7bb-47a5-bbf2-a307f15f63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trading data for AAPL for 6 months period\n",
    "p2_portfolio_df = project2_alpaca.get_bars(\n",
    "    tickers,\n",
    "    timeframe,\n",
    "    start = start,\n",
    "    end = end\n",
    ").df\n",
    "\n",
    "# Display sample data\n",
    "p2_portfolio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678877a8-765e-4f43-86e4-c7db932a6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bohek')\n",
    "p2_portfolio_df['close'].hvplot(rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbf59b-2f68-4fab-8d74-c8d2eef34fbd",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate Returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c2593-3c5a-472c-ace8-0d1f4ac71b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily returns using the closing prices and the pct_change function\n",
    "p2_portfolio_df['daily_returns'] = p2_portfolio_df['close'].pct_change()\n",
    "\n",
    "# Calculate the daily returns lagged using the daily returns\n",
    "p2_portfolio_df['daily_returns_lagged'] = p2_portfolio_df['daily_returns'].shift(1)\n",
    "\n",
    "# Drop all NaN values from the DataFrame\n",
    "p2_portfolio_df = p2_portfolio_df.dropna()\n",
    "\n",
    "# Display sample data\n",
    "p2_portfolio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f5f88-cd7a-40e0-b65d-84cda5cb2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_portfolio_df[['daily_returns','daily_returns_lagged']].corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c677a-da02-4b3b-a063-d02abae71fd9",
   "metadata": {},
   "source": [
    "## Define a Simple Momentum Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caead2e-0bcb-48d2-94e0-fb2172599516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new Signal column\n",
    "p2_portfolio_df['signal'] = 0.0\n",
    "\n",
    "# When Actual Returns are greater than or equal to 0, generate signal to buy stock long\n",
    "p2_portfolio_df.loc[(p2_portfolio_df['daily_returns'] >= 0), 'signal'] = 1\n",
    "\n",
    "# When Actual Returns are less than 0, generate signal to sell stock short\n",
    "p2_portfolio_df.loc[(p2_portfolio_df['daily_returns'] < 0), 'signal'] = -1\n",
    "\n",
    "# Drop JPM\n",
    "p2_portfolio_df = p2_portfolio_df.drop(['symbol'],axis=1)\n",
    "\n",
    "# Review the DataFrame\n",
    "display(p2_portfolio_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7cc148-2640-47bb-aabc-0cb72a76268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p2_portfolio_df = p2_portfolio_df.reset_index(drop=True)\n",
    "#p2_portfolio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287f0df-5748-47c3-ace0-80a10ea08745",
   "metadata": {},
   "source": [
    "## Create the features (`X`) and target (`y`) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285b5df-4bbd-45c2-85f0-51595002f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target set y using the signal column\n",
    "y = p2_portfolio_df['signal']\n",
    "\n",
    "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
    "X = p2_portfolio_df.drop(columns=[\"signal\"])\n",
    "\n",
    "# Review the features DataFrame\n",
    "X.head()\n",
    "\n",
    "# Display a sample of y and X features\n",
    "display(y.head())\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae15d9-c232-4237-8c35-a2e84603ea52",
   "metadata": {},
   "source": [
    "## Split the features and target sets into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ee5ef-c25d-4a14-9bac-9bedffc5791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and testing dataset and Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e9835-c931-4e1f-bc11-d7b2eb5b2db3",
   "metadata": {},
   "source": [
    "## Use scikit-learn's `StandardScaler` to scale the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dd933-ede7-4f40-80ca-12990a75d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "len(X_train_scaled[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5bfe9-6d43-403d-a7ec-5bfa53493d17",
   "metadata": {},
   "source": [
    "## Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef841944-bf23-4928-8abf-9b6750510ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the the number of inputs to the model\n",
    "number_inputs = 9\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 = 10\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn = Sequential()\n",
    "\n",
    "# Add the first Dense layer specifying the number of inputs, the number of hidden nodes, and the activation function\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_inputs, activation=\"relu\"))\n",
    "\n",
    "# Add the second Dense layer specifying the number of hidden nodes and the activation function\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787c7c8-f81a-46c5-8828-928aac62e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86767f0e-49bb-4348-a250-60de2a19fd73",
   "metadata": {},
   "source": [
    "## Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9926c3-79bb-4282-a63f-ea291a5d4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "#nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3587ff-3724-41b0-84ba-e446d0475f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632345c-468e-4ba6-8224-192fc058d4c9",
   "metadata": {},
   "source": [
    "## Evaluate the model using the test data to determine the model’s loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4081bf-fa08-432f-991e-65fb82f24c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdf114-8063-43e9-be41-2dce63a8c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions on the test data\n",
    "predictions = nn.predict(X_test_scaled)\n",
    "hv.extension('bokeh')\n",
    "pd.DataFrame(predictions).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea370a-2fd8-4dd7-a0cc-1af06fc10ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions on the test data\n",
    "predictions = nn.predict(X_test_scaled)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72883b98-a6cf-4c5c-9366-2ec97778b7f8",
   "metadata": {},
   "source": [
    " ### Making Predictions using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2451f-0701-40ac-827d-bf3118594515",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sorted_X_test = X_test.sort_index()\n",
    "X_test\n",
    "display(Sorted_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de35e6-2ee3-4fde-a67a-a8e5e25cedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume the prediction > mean is buy, < mean is sell, =mean is do nothing\n",
    "instr_df = pd.DataFrame()\n",
    "mean_pred = predictions.mean()\n",
    "pred_at = 0\n",
    "for index,row in X_test.iterrows():\n",
    "    pred = predictions[pred_at]\n",
    "    pred_at += 1\n",
    "    if pred > mean_pred:\n",
    "      signal = 1\n",
    "    elif pred < mean_pred:\n",
    "      signal = -1\n",
    "    else:\n",
    "      signal = 0\n",
    "\n",
    "    instr_df = pd.concat([instr_df, pd.DataFrame([[index.date(), 'AAPL', signal]])])\n",
    "instr_df.sort_values(0, inplace=True)\n",
    "\n",
    "display(instr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9e72f-333f-4393-aae1-2084239ec2f0",
   "metadata": {},
   "source": [
    " ### Show result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb73fc3-3f56-4677-a8ce-67b1dd072456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trade import trade_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6285055-a210-49b9-8ab7-b3d8a6ea59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "trade_action(instr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48221e4b-df1c-43a6-9d31-7a5cc6d4e2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
